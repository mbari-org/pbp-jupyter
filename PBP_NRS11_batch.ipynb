{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d087f632",
   "metadata": {
    "id": "d087f632"
   },
   "source": [
    "# Exercising PBP/PyPAM on NRS11 data\n",
    "\n",
    "The main steps in this notebook are:\n",
    "\n",
    "- Check PBP package\n",
    "- Do preparations in terms of working space for downloaded and generated files\n",
    "- Generate HMB for a single day\n",
    "- Generate HMB for multiple days in parallel using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b81f4-90c6-4910-8f63-f66ae0013afc",
   "metadata": {
    "id": "879b81f4-90c6-4910-8f63-f66ae0013afc"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1afecf4-488f-4d0d-853e-6f59214ebe2f",
   "metadata": {
    "id": "c1afecf4-488f-4d0d-853e-6f59214ebe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PBP version   : 1.0.10\n",
      "PyPAM version : 0.3.0\n"
     ]
    }
   ],
   "source": [
    "# Basic check that the PBP package is in place:\n",
    "# (This package brings in all associated dependencies)\n",
    "import pbp\n",
    "print(f\"PBP version   : {pbp.get_pbp_version()}\")\n",
    "print(f\"PyPAM version : {pbp.get_pypam_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb10e07-b1dd-4a18-979c-1289730746fa",
   "metadata": {
    "id": "9fb10e07-b1dd-4a18-979c-1289730746fa"
   },
   "source": [
    "## Code imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c1c775-5d74-45f7-be76-e82c1d35286e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "27c1c775-5d74-45f7-be76-e82c1d35286e",
    "outputId": "d118c888-e562-4197-bd24-165399dabc36"
   },
   "outputs": [],
   "source": [
    "from pbp.process_helper import ProcessHelper\n",
    "from pbp.file_helper import FileHelper\n",
    "from pbp.logging_helper import create_logger\n",
    "from pbp import get_pbp_version\n",
    "\n",
    "from google.cloud.storage import Client as GsClient\n",
    "\n",
    "import xarray as xr\n",
    "import dask\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8dc4d-97c2-4e40-a4f7-845d7e716855",
   "metadata": {
    "id": "6fe8dc4d-97c2-4e40-a4f7-845d7e716855"
   },
   "source": [
    "## Some parameters for PBP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i-bF81y_WO_M",
   "metadata": {
    "id": "i-bF81y_WO_M"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9whAGckWQYX",
   "metadata": {
    "id": "e9whAGckWQYX"
   },
   "outputs": [],
   "source": [
    "# TODO: update this when we incorporate the JSON generation\n",
    "json_base_dir        = 'NRS11/noaa-passive-bioacoustic_nrs_11_2019-2021'\n",
    "\n",
    "global_attrs_uri     = 'NRS11/INPUT/globalAttributes_NRS11.yaml'\n",
    "variable_attrs_uri   = 'NRS11/INPUT/variableAttributes_NRS11.yaml'\n",
    "\n",
    "voltage_multiplier   = 2.5\n",
    "sensitivity_uri      = 'NRS11/INPUT/NRS11_H5R6_sensitivity_hms5kHz.nc'\n",
    "subset_to            = (10, 2_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-5cgQfDkWWps",
   "metadata": {
    "id": "-5cgQfDkWWps"
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9f38450-5afd-44e8-a55e-9a3861c61fce",
   "metadata": {
    "id": "d9f38450-5afd-44e8-a55e-9a3861c61fce"
   },
   "outputs": [],
   "source": [
    "# Downloaded files are stored here while being processed:\n",
    "download_dir         = 'NRS11/DOWNLOADS'\n",
    "\n",
    "# Location for generated files:\n",
    "output_dir           = 'NRS11/OUTPUT'\n",
    "# A prefix for the name of generate files:\n",
    "output_prefix         = 'NRS11_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e4acf-b060-40d3-9009-f6c94d1e0d84",
   "metadata": {
    "id": "421e4acf-b060-40d3-9009-f6c94d1e0d84"
   },
   "source": [
    "# Supporting functions\n",
    "\n",
    "PBP includes these two main modules that we will be using below:\n",
    "\n",
    "- `FileHelper`: Facilitates input file reading. It supports reading local files as well as from GCP (`gs://` URIs) and AWS (`s3://` URIs).\n",
    "- `ProcessHelper`: The main processing module.\n",
    "\n",
    "We first define a function that takes care of HMB generation for a given date.\n",
    "\n",
    "Based on that function, we then define one other function to dispatch multiple dates in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f949e2-732a-4e32-a68b-f3e2ea755f6b",
   "metadata": {
    "id": "36f949e2-732a-4e32-a68b-f3e2ea755f6b"
   },
   "source": [
    "## A function to process a given day\n",
    "\n",
    "Supported by those PBP modules, we define a function that takes care of processing a given day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d440360",
   "metadata": {
    "id": "9d440360"
   },
   "outputs": [],
   "source": [
    "def process_date(date: str, gen_netcdf: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to generate the HMB product for a given day.\n",
    "\n",
    "    It makes use of supporting elements in PBP in terms of logging,\n",
    "    file handling, and PyPAM based HMB generation.\n",
    "\n",
    "    :param date: Date to process, in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated\n",
    "    (see parallel execution below).\n",
    "\n",
    "    :return: the generated xarray dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    log_filename = f\"{output_dir}/{output_prefix}{date}.log\"\n",
    "\n",
    "    log = create_logger(\n",
    "        log_filename_and_level=(log_filename, \"INFO\"),\n",
    "        console_level=None,\n",
    "    )\n",
    "\n",
    "    # we are only downloading publicly accessible datasets:\n",
    "    gs_client = GsClient.create_anonymous_client()\n",
    "\n",
    "    file_helper = FileHelper(\n",
    "        log=log,\n",
    "        json_base_dir=json_base_dir,\n",
    "        gs_client=gs_client,\n",
    "        download_dir=download_dir,\n",
    "    )\n",
    "\n",
    "    process_helper = ProcessHelper(\n",
    "        log=log,\n",
    "        file_helper=file_helper,\n",
    "        output_dir=output_dir,\n",
    "        output_prefix=output_prefix,\n",
    "        gen_netcdf=gen_netcdf,\n",
    "        global_attrs_uri=global_attrs_uri,\n",
    "        variable_attrs_uri=variable_attrs_uri,\n",
    "        voltage_multiplier=voltage_multiplier,\n",
    "        sensitivity_uri=sensitivity_uri,\n",
    "        subset_to=subset_to,\n",
    "    )\n",
    "\n",
    "    ## now, get the HMB result:\n",
    "    print(f'::: Started processing {date=}')\n",
    "    result = process_helper.process_day(date)\n",
    "\n",
    "    if gen_netcdf:\n",
    "        nc_filename = f\"{output_dir}/{output_prefix}{date}.nc\"\n",
    "        print(f':::   Ended processing {date=} =>  {nc_filename=}')\n",
    "    else:\n",
    "        print(f':::   Ended processing {date=} => (dataset generated in memory)')\n",
    "\n",
    "    if result is not None:\n",
    "        return result.dataset\n",
    "    else:\n",
    "        print(f'::: UNEXPECTED: no segments were processed for {date=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ecb94-c78f-4986-a029-80d2924f1481",
   "metadata": {
    "id": "218ecb94-c78f-4986-a029-80d2924f1481"
   },
   "source": [
    "## A function to process multiple days\n",
    "\n",
    "We use [Dask](https://examples.dask.org/delayed.html) to dispatch, in parallel, multiple instances of the `process_date` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f820f99-e302-475d-b858-bb5dc806c8c1",
   "metadata": {
    "id": "9f820f99-e302-475d-b858-bb5dc806c8c1"
   },
   "outputs": [],
   "source": [
    "def process_multiple_dates(dates: list[str], gen_netcdf: bool = False) -> list[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Generates HMB for multiple days in parallel using Dask.\n",
    "    Returns the resulting HMB datasets.\n",
    "\n",
    "    :param dates: The dates to process, each in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated.\n",
    "\n",
    "    :return: the list of generated datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def delayed_process_date(date: str):\n",
    "        return process_date(date, gen_netcdf=gen_netcdf)\n",
    "\n",
    "    ## To display total elapsed time at the end the processing:\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## This will be called by Dask when all dates have completed processing:\n",
    "    def aggregate(*datasets) -> list[xr.Dataset]:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'===> All {len(datasets)} dates completed. Elapsed time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} mins)')\n",
    "        return datasets\n",
    "\n",
    "\n",
    "    ## Prepare the processes:\n",
    "    delayed_processes = [delayed_process_date(date) for date in dates]\n",
    "    aggregation = dask.delayed(aggregate)(*delayed_processes)\n",
    "\n",
    "    ## And launch them:\n",
    "    return aggregation.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cee960-d5e4-4574-8dd3-f5f30d772869",
   "metadata": {
    "id": "71cee960-d5e4-4574-8dd3-f5f30d772869"
   },
   "source": [
    "# Generating the HMB products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16561f4a-cd90-4762-8a1d-9072b436f753",
   "metadata": {
    "id": "16561f4a-cd90-4762-8a1d-9072b436f753"
   },
   "source": [
    "## Processing a single day\n",
    "\n",
    "In general, we are more interested in processing multiple dates, but we could process a single date by just calling `process_date` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99d7252d-45e2-46e8-9379-f072d5d674c5",
   "metadata": {
    "id": "99d7252d-45e2-46e8-9379-f072d5d674c5"
   },
   "outputs": [],
   "source": [
    "## Just uncomment the following line:\n",
    "# process_date('20200101')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e2ffd-6f67-425c-bd2f-dfbd5dd702e9",
   "metadata": {
    "id": "567e2ffd-6f67-425c-bd2f-dfbd5dd702e9"
   },
   "source": [
    "## Processing multiple days\n",
    "\n",
    "We use the `process_multiple_dates` defined above to launch the generation of multiple HMB datasets in parallel.\n",
    "\n",
    "**NOTE**:\n",
    "- Included JSON files in the current PBP image only cover Jan 01–31, 2020.\n",
    "- Such JSON files could alternatively be located in external buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd9d6c50-687d-4b5f-9987-7abb0dd8c848",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd9d6c50-687d-4b5f-9987-7abb0dd8c848",
    "outputId": "dbe88a52-d27c-4a67-ced7-d68b6d0294bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20200101',\n",
       " '20200102',\n",
       " '20200103',\n",
       " '20200104',\n",
       " '20200105',\n",
       " '20200106',\n",
       " '20200107',\n",
       " '20200108',\n",
       " '20200109',\n",
       " '20200110']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here, we set `dates` as the list of 'YYYYMMDD' dates we want to process:\n",
    "## Let's use pandas to help generate the list:\n",
    "date_range = pd.date_range(start='2020-01-01', end='2020-01-10')\n",
    "dates = date_range.strftime('%Y%m%d').tolist()\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c85cf420",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c85cf420",
    "outputId": "c7d37240-968a-48a8-ee37-7a2097c8378b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching HMB generation for 10 dates=['20200101', '20200102', '20200103', '20200104', '20200105', '20200106', '20200107', '20200108', '20200109', '20200110']\n",
      "::: Started processing date='20200104'\n",
      "::: Started processing date='20200107'\n",
      "::: Started processing date='20200103'\n",
      "::: Started processing date='20200108'\n",
      "::: Started processing date='20200106'\n",
      "::: Started processing date='20200110'\n",
      "::: Started processing date='20200109'\n",
      ":::   Ended processing date='20200109' => (dataset generated in memory)\n",
      ":::   Ended processing date='20200104' => (dataset generated in memory)\n",
      ":::   Ended processing date='20200108' => (dataset generated in memory)\n",
      ":::   Ended processing date='20200103' => (dataset generated in memory)\n",
      ":::   Ended processing date='20200106' => (dataset generated in memory)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SoundStatus' object has no attribute 'sound_file'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m gen_netcdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m   \u001B[38;5;66;03m# True for each process to save its generated file.\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m## NOTE: set to True in new binder environment to see whether the issues persist.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m## Get all HMB datasets:\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m generated_datasets \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_multiple_dates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgen_netcdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_netcdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGenerated datasets: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(generated_datasets)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gen_netcdf:\n",
      "Cell \u001B[0;32mIn[17], line 33\u001B[0m, in \u001B[0;36mprocess_multiple_dates\u001B[0;34m(dates, gen_netcdf)\u001B[0m\n\u001B[1;32m     30\u001B[0m aggregation \u001B[38;5;241m=\u001B[39m dask\u001B[38;5;241m.\u001B[39mdelayed(aggregate)(\u001B[38;5;241m*\u001B[39mdelayed_processes)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m## And launch them:\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43maggregation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/dask/base.py:342\u001B[0m, in \u001B[0;36mDaskMethodsMixin.compute\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    319\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute this dask collection\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \n\u001B[1;32m    321\u001B[0m \u001B[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;124;03m    dask.compute\u001B[39;00m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 342\u001B[0m     (result,) \u001B[38;5;241m=\u001B[39m \u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/dask/base.py:628\u001B[0m, in \u001B[0;36mcompute\u001B[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001B[0m\n\u001B[1;32m    625\u001B[0m     postcomputes\u001B[38;5;241m.\u001B[39mappend(x\u001B[38;5;241m.\u001B[39m__dask_postcompute__())\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m shorten_traceback():\n\u001B[0;32m--> 628\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdsk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repack([f(r, \u001B[38;5;241m*\u001B[39ma) \u001B[38;5;28;01mfor\u001B[39;00m r, (f, a) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(results, postcomputes)])\n",
      "Cell \u001B[0;32mIn[17], line 16\u001B[0m, in \u001B[0;36mprocess_multiple_dates.<locals>.delayed_process_date\u001B[0;34m(date)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;129m@dask\u001B[39m\u001B[38;5;241m.\u001B[39mdelayed\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdelayed_process_date\u001B[39m(date: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mprocess_date\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgen_netcdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_netcdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[16], line 49\u001B[0m, in \u001B[0;36mprocess_date\u001B[0;34m(date, gen_netcdf)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m## now, get the HMB result:\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m::: Started processing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdate\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 49\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_helper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_day\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gen_netcdf:\n\u001B[1;32m     52\u001B[0m     nc_filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mdate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.nc\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/pbp/process_helper.py:236\u001B[0m, in \u001B[0;36mProcessHelper.process_day\u001B[0;34m(self, date)\u001B[0m\n\u001B[1;32m    233\u001B[0m     save_dataset_to_netcdf(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog, ds_result, nc_filename)\n\u001B[1;32m    234\u001B[0m     generated_filenames\u001B[38;5;241m.\u001B[39mappend(nc_filename)\n\u001B[0;32m--> 236\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_helper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mday_completed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ProcessDayResult(generated_filenames, ds_result)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/pbp/file_helper.py:311\u001B[0m, in \u001B[0;36mFileHelper.day_completed\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;124;03mSince a process is launched only for day, we simply clear the cache.\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m# first, close all sound files still open:\u001B[39;00m\n\u001B[0;32m--> 311\u001B[0m sound_files_open \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mc_ss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mc_ss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msound_cache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mc_ss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msound_file\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    313\u001B[0m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sound_files_open) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    316\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mday_completed: closing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(sound_files_open)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m sound files still open\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    317\u001B[0m     )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/pbp/file_helper.py:312\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;124;03mSince a process is launched only for day, we simply clear the cache.\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m# first, close all sound files still open:\u001B[39;00m\n\u001B[1;32m    311\u001B[0m sound_files_open \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 312\u001B[0m     c_ss \u001B[38;5;28;01mfor\u001B[39;00m c_ss \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msound_cache\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mc_ss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msound_file\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    313\u001B[0m ]\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sound_files_open) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    316\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mday_completed: closing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(sound_files_open)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m sound files still open\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    317\u001B[0m     )\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'SoundStatus' object has no attribute 'sound_file'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::   Ended processing date='20200110' => (dataset generated in memory)\n",
      ":::   Ended processing date='20200107' => (dataset generated in memory)\n"
     ]
    }
   ],
   "source": [
    "## Now, launch the generation:\n",
    "\n",
    "print(f\"Launching HMB generation for {len(dates)} {dates=}\")\n",
    "\n",
    "## NOTE: due to issues observed when concurrently saving the resulting netCDF files,\n",
    "## this flag allows to postpone the saving for after all datasets have been generated:\n",
    "gen_netcdf = False   # True for each process to save its generated file.\n",
    "\n",
    "## Get all HMB datasets:\n",
    "generated_datasets = process_multiple_dates(dates, gen_netcdf=gen_netcdf)\n",
    "\n",
    "print(f'Generated datasets: {len(generated_datasets)}\\n')\n",
    "\n",
    "if gen_netcdf:\n",
    "    print('DONE. Datasets should have been saved already.')\n",
    "else:\n",
    "    # Do the file saving here:\n",
    "    print('Saving generated datasets...')\n",
    "    for date, ds in zip(dates, generated_datasets):\n",
    "        nc_filename = f'{output_dir}/{output_prefix}{date}.nc'\n",
    "        print(f'  - Saving {nc_filename=}')\n",
    "        try:\n",
    "            ds.to_netcdf(nc_filename,\n",
    "                         engine=\"h5netcdf\",\n",
    "                         encoding={\n",
    "                            \"effort\": {\"_FillValue\": None},\n",
    "                            \"frequency\": {\"_FillValue\": None},\n",
    "                            \"sensitivity\": {\"_FillValue\": None},\n",
    "                         },\n",
    "            )\n",
    "        except Exception as e:  # pylint: disable=broad-exception-caught\n",
    "            print(f\"         Unable to save {nc_filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "824e943e-253d-4e5a-b1ac-4a673998dc98",
   "metadata": {
    "id": "824e943e-253d-4e5a-b1ac-4a673998dc98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 84\n",
      "-rw-r--r-- 1 jovyan users 8585 Jul 18 21:14 NRS11_20200101.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200102.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200103.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200104.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200105.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200106.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200107.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200108.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200109.log\n",
      "-rw-r--r-- 1 jovyan users 4699 Jul 18 21:14 NRS11_20200110.log\n"
     ]
    }
   ],
   "source": [
    "!ls -l NRS11/OUTPUT"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
