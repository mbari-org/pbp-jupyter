{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d087f632",
   "metadata": {
    "id": "d087f632"
   },
   "source": [
    "# Exercising PBP/PyPAM on NRS11 data\n",
    "\n",
    "The main steps in this notebook are:\n",
    "\n",
    "- Check PBP package\n",
    "- Do preparations in terms of working space for downloaded and generated files\n",
    "- Generate HMB for a single day\n",
    "- Generate HMB for multiple days in parallel using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b81f4-90c6-4910-8f63-f66ae0013afc",
   "metadata": {
    "id": "879b81f4-90c6-4910-8f63-f66ae0013afc"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1afecf4-488f-4d0d-853e-6f59214ebe2f",
   "metadata": {
    "id": "c1afecf4-488f-4d0d-853e-6f59214ebe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PBP version   : 1.0.8\n",
      "PyPAM version : 0.3.0\n"
     ]
    }
   ],
   "source": [
    "# Basic check that the PBP package is in place:\n",
    "# (This package brings in all associated dependencies)\n",
    "import pbp\n",
    "print(f\"PBP version   : {pbp.get_pbp_version()}\")\n",
    "print(f\"PyPAM version : {pbp.get_pypam_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb10e07-b1dd-4a18-979c-1289730746fa",
   "metadata": {
    "id": "9fb10e07-b1dd-4a18-979c-1289730746fa"
   },
   "source": [
    "## Code imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c1c775-5d74-45f7-be76-e82c1d35286e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "27c1c775-5d74-45f7-be76-e82c1d35286e",
    "outputId": "d118c888-e562-4197-bd24-165399dabc36"
   },
   "outputs": [],
   "source": [
    "from pbp.process_helper import ProcessHelper\n",
    "from pbp.file_helper import FileHelper\n",
    "from pbp.logging_helper import create_logger\n",
    "from pbp import get_pbp_version\n",
    "\n",
    "from google.cloud.storage import Client as GsClient\n",
    "\n",
    "import xarray as xr\n",
    "import dask\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8dc4d-97c2-4e40-a4f7-845d7e716855",
   "metadata": {
    "id": "6fe8dc4d-97c2-4e40-a4f7-845d7e716855"
   },
   "source": [
    "## Some parameters for PBP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i-bF81y_WO_M",
   "metadata": {
    "id": "i-bF81y_WO_M"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9whAGckWQYX",
   "metadata": {
    "id": "e9whAGckWQYX"
   },
   "outputs": [],
   "source": [
    "# TODO: update this when we incorporate the JSON generation\n",
    "json_base_dir        = 'NRS11/noaa-passive-bioacoustic_nrs_11_2019-2021'\n",
    "\n",
    "global_attrs_uri     = 'NRS11/INPUT/globalAttributes_NRS11.yaml'\n",
    "variable_attrs_uri   = 'NRS11/INPUT/variableAttributes_NRS11.yaml'\n",
    "\n",
    "voltage_multiplier   = 2.5\n",
    "sensitivity_uri      = 'NRS11/INPUT/NRS11_H5R6_sensitivity_hms5kHz.nc'\n",
    "subset_to            = (10, 2_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-5cgQfDkWWps",
   "metadata": {
    "id": "-5cgQfDkWWps"
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f38450-5afd-44e8-a55e-9a3861c61fce",
   "metadata": {
    "id": "d9f38450-5afd-44e8-a55e-9a3861c61fce"
   },
   "outputs": [],
   "source": [
    "# Downloaded files are stored here while being processed:\n",
    "download_dir         = 'NRS11/DOWNLOADS'\n",
    "\n",
    "# Location for generated files:\n",
    "output_dir           = 'NRS11/OUTPUT'\n",
    "# A prefix for the name of generate files:\n",
    "output_prefix         = 'NRS11_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e4acf-b060-40d3-9009-f6c94d1e0d84",
   "metadata": {
    "id": "421e4acf-b060-40d3-9009-f6c94d1e0d84"
   },
   "source": [
    "# Supporting functions\n",
    "\n",
    "PBP includes these two main modules that we will be using below:\n",
    "\n",
    "- `FileHelper`: Facilitates input file reading. It supports reading local files as well as from GCP (`gs://` URIs) and AWS (`s3://` URIs).\n",
    "- `ProcessHelper`: The main processing module.\n",
    "\n",
    "We first define a function that takes care of HMB generation for a given date.\n",
    "\n",
    "Based on that function, we then define one other function to dispatch multiple dates in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f949e2-732a-4e32-a68b-f3e2ea755f6b",
   "metadata": {
    "id": "36f949e2-732a-4e32-a68b-f3e2ea755f6b"
   },
   "source": [
    "## A function to process a given day\n",
    "\n",
    "Supported by those PBP modules, we define a function that takes care of processing a given day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d440360",
   "metadata": {
    "id": "9d440360"
   },
   "outputs": [],
   "source": [
    "def process_date(date: str, gen_netcdf: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to generate the HMB product for a given day.\n",
    "\n",
    "    It makes use of supporting elements in PBP in terms of logging,\n",
    "    file handling, and PyPAM based HMB generation.\n",
    "\n",
    "    :param date: Date to process, in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated\n",
    "    (see parallel execution below).\n",
    "\n",
    "    :return: the generated xarray dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    log_filename = f\"{output_dir}/{output_prefix}{date}.log\"\n",
    "\n",
    "    log = create_logger(\n",
    "        log_filename_and_level=(log_filename, \"INFO\"),\n",
    "        console_level=None,\n",
    "    )\n",
    "\n",
    "    # we are only downloading publicly accessible datasets:\n",
    "    gs_client = GsClient.create_anonymous_client()\n",
    "\n",
    "    file_helper = FileHelper(\n",
    "        log=log,\n",
    "        json_base_dir=json_base_dir,\n",
    "        gs_client=gs_client,\n",
    "        download_dir=download_dir,\n",
    "    )\n",
    "\n",
    "    process_helper = ProcessHelper(\n",
    "        log=log,\n",
    "        file_helper=file_helper,\n",
    "        output_dir=output_dir,\n",
    "        output_prefix=output_prefix,\n",
    "        global_attrs_uri=global_attrs_uri,\n",
    "        variable_attrs_uri=variable_attrs_uri,\n",
    "        voltage_multiplier=voltage_multiplier,\n",
    "        sensitivity_uri=sensitivity_uri,\n",
    "        subset_to=subset_to,\n",
    "    )\n",
    "\n",
    "    ## now, get the HMB result:\n",
    "    print(f'::: Started processing {date=}')\n",
    "    result = process_helper.process_day(date)\n",
    "\n",
    "    if gen_netcdf:\n",
    "        nc_filename = f\"{output_dir}/{output_prefix}{date}.nc\"\n",
    "        print(f':::   Ended processing {date=} =>  {nc_filename=}')\n",
    "    else:\n",
    "        print(f':::   Ended processing {date=} => (dataset generated in memory)')\n",
    "\n",
    "    if result is not None:\n",
    "        return result.dataset\n",
    "    else:\n",
    "        print(f'::: UNEXPECTED: no segments were processed for {date=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ecb94-c78f-4986-a029-80d2924f1481",
   "metadata": {
    "id": "218ecb94-c78f-4986-a029-80d2924f1481"
   },
   "source": [
    "## A function to process multiple days\n",
    "\n",
    "We use [Dask](https://examples.dask.org/delayed.html) to dispatch, in parallel, multiple instances of the `process_date` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f820f99-e302-475d-b858-bb5dc806c8c1",
   "metadata": {
    "id": "9f820f99-e302-475d-b858-bb5dc806c8c1"
   },
   "outputs": [],
   "source": [
    "def process_multiple_dates(dates: list[str], gen_netcdf: bool = False) -> list[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Generates HMB for multiple days in parallel using Dask.\n",
    "    Returns the resulting HMB datasets.\n",
    "\n",
    "    :param dates: The dates to process, each in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated.\n",
    "\n",
    "    :return: the list of generated datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def delayed_process_date(date: str):\n",
    "        return process_date(date, gen_netcdf=gen_netcdf)\n",
    "\n",
    "    ## To display total elapsed time at the end the processing:\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## This will be called by Dask when all dates have completed processing:\n",
    "    def aggregate(*datasets) -> list[xr.Dataset]:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'===> All {len(datasets)} dates completed. Elapsed time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} mins)')\n",
    "        return datasets\n",
    "\n",
    "\n",
    "    ## Prepare the processes:\n",
    "    delayed_processes = [delayed_process_date(date) for date in dates]\n",
    "    aggregation = dask.delayed(aggregate)(*delayed_processes)\n",
    "\n",
    "    ## And launch them:\n",
    "    return aggregation.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cee960-d5e4-4574-8dd3-f5f30d772869",
   "metadata": {
    "id": "71cee960-d5e4-4574-8dd3-f5f30d772869"
   },
   "source": [
    "# Generating the HMB products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16561f4a-cd90-4762-8a1d-9072b436f753",
   "metadata": {
    "id": "16561f4a-cd90-4762-8a1d-9072b436f753"
   },
   "source": [
    "## Processing a single day\n",
    "\n",
    "In general, we are more interested in processing multiple dates, but we could process a single date by just calling `process_date` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d7252d-45e2-46e8-9379-f072d5d674c5",
   "metadata": {
    "id": "99d7252d-45e2-46e8-9379-f072d5d674c5"
   },
   "outputs": [],
   "source": [
    "## Just uncomment the following line:\n",
    "# process_date('20200101')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e2ffd-6f67-425c-bd2f-dfbd5dd702e9",
   "metadata": {
    "id": "567e2ffd-6f67-425c-bd2f-dfbd5dd702e9"
   },
   "source": [
    "## Processing multiple days\n",
    "\n",
    "We use the `process_multiple_dates` defined above to launch the generation of multiple HMB datasets in parallel.\n",
    "\n",
    "**NOTE**:\n",
    "- Included JSON files in the current PBP image only cover Jan 01â€“31, 2020.\n",
    "- Such JSON files could alternatively be located in external buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd9d6c50-687d-4b5f-9987-7abb0dd8c848",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd9d6c50-687d-4b5f-9987-7abb0dd8c848",
    "outputId": "dbe88a52-d27c-4a67-ced7-d68b6d0294bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20200104', '20200105']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here, we set `dates` as the list of 'YYYYMMDD' dates we want to process:\n",
    "\n",
    "## For just a few dates, we can define the list explicitly:\n",
    "dates = ['20200101', '20200102', '20200103']\n",
    "\n",
    "## but in general we can use pandas to help us generate the list:\n",
    "# date_range = pd.date_range(start='2020-01-01', end='2020-01-31')\n",
    "# dates = date_range.strftime('%Y%m%d').tolist()\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85cf420",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c85cf420",
    "outputId": "c7d37240-968a-48a8-ee37-7a2097c8378b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching HMB generation for 2 dates=['20200104', '20200105']\n",
      "Generated datasets: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Now, launch the generation:\n",
    "\n",
    "print(f\"Launching HMB generation for {len(dates)} {dates=}\")\n",
    "\n",
    "## NOTE: due to issues observed when concurrently saving the resulting netCDF files,\n",
    "## this flag allows to postpone the saving for after all datasets have been generated:\n",
    "gen_netcdf = True   # True for each process to save its generated file.\n",
    "## NOTE: set to True in new binder environment to see whether the issues persist.\n",
    "\n",
    "## Get all HMB datasets:\n",
    "generated_datasets = process_multiple_dates(dates, gen_netcdf=gen_netcdf)\n",
    "\n",
    "print(f'Generated datasets: {len(generated_datasets)}\\n')\n",
    "\n",
    "if gen_netcdf:\n",
    "    print('DONE. Datasets should have been saved already.')\n",
    "else:\n",
    "    # so, we now do the file saving here:\n",
    "    print('Saving generated datasets...')\n",
    "    for date, ds in zip(dates, generated_datasets):\n",
    "        nc_filename = f'{output_dir}/{output_prefix}{date}.nc'\n",
    "        print(f'  Saving {nc_filename=}')\n",
    "        try:\n",
    "            ds.to_netcdf(nc_filename,\n",
    "                         engine=\"netcdf4\",\n",
    "                         encoding={\n",
    "                            \"effort\": {\"_FillValue\": None},\n",
    "                            \"frequency\": {\"_FillValue\": None},\n",
    "                            \"sensitivity\": {\"_FillValue\": None},\n",
    "                         },\n",
    "            )\n",
    "        except Exception as e:  # pylint: disable=broad-exception-caught\n",
    "            print(f\"Unable to save {nc_filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824e943e-253d-4e5a-b1ac-4a673998dc98",
   "metadata": {
    "id": "824e943e-253d-4e5a-b1ac-4a673998dc98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing *.nc in OUTPUT folder:\n",
      "-rw-r--r-- 1 jovyan jovyan   12288 Jul 17 23:58 NRS11/OUTPUT/NRS11_20200101.nc\n",
      "-rw-r--r-- 1 jovyan jovyan   12288 Jul 17 23:58 NRS11/OUTPUT/NRS11_20200102.nc\n",
      "-rw-r--r-- 1 jovyan jovyan   12288 Jul 17 23:58 NRS11/OUTPUT/NRS11_20200103.nc\n",
      "-rw-r--r-- 1 jovyan jovyan 6316672 Jul 18 00:00 NRS11/OUTPUT/NRS11_20200104.nc\n",
      "-rw-r--r-- 1 jovyan jovyan 6316672 Jul 17 23:59 NRS11/OUTPUT/NRS11_20200105.nc\n",
      "-rw-r--r-- 1 jovyan jovyan   12288 Jul 17 23:39 NRS11/OUTPUT/NRS11_20200110.nc\n"
     ]
    }
   ],
   "source": [
    "print('\\nListing *.nc in OUTPUT folder:')\n",
    "!ls -l NRS11/OUTPUT/*.nc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
